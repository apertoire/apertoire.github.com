<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Unraid on Apertoire </title>
      <generator>Hugo</generator>
      <link>http://localhost:1313/tags/unraid/</link>
      <description>Articles about Technology in general</description>
    <language>en-us</language>
    
    
    
    
    <item>
      <title>Dockerization</title>
      <link>http://localhost:1313/dockeritazion/</link>
      <pubDate>Thu, 09 Jan 2014 23:23:23 GMT</pubDate>
      
      <guid>http://localhost:1313/dockeritazion/</guid>
      <description>

&lt;h2 id=&#34;background:7168b982f076d28b250775e790646ac1&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;so, &lt;a href=&#34;http://www.apertoire.net/storage-wars/&#34;&gt;back in 2012&lt;/a&gt; i posted about virtualizing my media storage and general backup infrastructure.&lt;/p&gt;

&lt;p&gt;today i will post about going back to bare metal &amp;hellip; well, sort of :)&lt;/p&gt;

&lt;p&gt;just a couple of days ago, &lt;a href=&#34;http://lime-technology.com/forum/index.php?topic=21514.0&#34;&gt;my setup&lt;/a&gt; was running and working mostly fine&lt;/p&gt;

&lt;p&gt;the biggest nuisance was that since i didn&amp;rsquo;t leave the unraid boxes running 24x7, each time i wanted to turn them on, i had to start the unraid vms, then turn the das boxes and finally start the array manually. kind of tedious.&lt;/p&gt;

&lt;p&gt;fast forward to 2013, esxi 5.5 is out and you start to hear that vsphere client will be crippled as far as new esxi versions are concerned.&lt;/p&gt;

&lt;p&gt;some discussions started showing up in the unraid forums, about virtualizing unraid under xen or kvm.&lt;/p&gt;

&lt;p&gt;the time was ripe for an overhaul of my infrastructure, get rid of esxi and move on to either xen or kvm. or so i thought.&lt;/p&gt;

&lt;h2 id=&#34;research:7168b982f076d28b250775e790646ac1&#34;&gt;Research&lt;/h2&gt;

&lt;p&gt;although a lot of topics were being discussed in the unraid forums, something that got to me, was the fact that actually i should be running unraid as close to the hardware as possible, which meant de-virtualizing my esxi vms.&lt;/p&gt;

&lt;p&gt;at about the same time, a new player entered the scene, which i&amp;rsquo;d heard about before, but never really understood&lt;/p&gt;

&lt;p&gt;enter &lt;a href=&#34;http://www.docker.io/&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;it made a huge difference to think in terms of containerized lightweight applications rather than full blown vms.&lt;/p&gt;

&lt;p&gt;i wouldn&amp;rsquo;t even need xen nor kvm, if i could get the apps i ran on my vms to run as docker containers.&lt;/p&gt;

&lt;p&gt;that&amp;rsquo;s what i set out to do, and that&amp;rsquo;s what i achieved.&lt;/p&gt;

&lt;p&gt;this is the story of what i did&lt;/p&gt;

&lt;h2 id=&#34;step-by-step:7168b982f076d28b250775e790646ac1&#34;&gt;Step by Step&lt;/h2&gt;

&lt;h3 id=&#34;devirtualize-unraids:7168b982f076d28b250775e790646ac1&#34;&gt;Devirtualize unraids&lt;/h3&gt;

&lt;p&gt;i removed 2 m1015s from my head server and installed them in two Asrock motherboards in each Norco case. I purchased the following components&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Motherboad: 2x &lt;a href=&#34;http://www.amazon.com/gp/product/B00FIWSIVS/ref=oh_details_o05_s00_i00?ie=UTF8&amp;amp;psc=1&#34;&gt;ASRock H87M PRO4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CPU: 2x &lt;a href=&#34;http://www.amazon.com/gp/product/B00EUVEFEC/ref=oh_details_o07_s00_i01?ie=UTF8&amp;amp;psc=1&#34;&gt;Intel I3-4130T&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RAM: 1X &lt;a href=&#34;http://www.amazon.com/gp/product/B006WAGGUK/ref=oh_details_o07_s00_i00?ie=UTF8&amp;amp;psc=1&#34;&gt;Crucial Ballistix Sport 8GB Kit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;set-up-the-new-virtualization-host:7168b982f076d28b250775e790646ac1&#34;&gt;Set up the new virtualization host&lt;/h3&gt;

&lt;p&gt;installed Ubuntu 12.04.3 LTS, with zfs, docker and kvm support. hard disks were used as follows&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
    &lt;td&gt;Qty. &amp;nbsp;&lt;/td&gt;
    &lt;td&gt;Item&lt;/td&gt;
    &lt;td&gt;Filesystem/Layout&lt;/td&gt;
    &lt;td&gt;Usage&lt;/td&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
    &lt;td&gt;1x&lt;/td&gt;
    &lt;td&gt;120Gb SSD &amp;nbsp;&lt;/td&gt;
    &lt;td&gt;ext4&lt;/td&gt;
    &lt;td&gt;boot and general purpose (/home, etc.)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;2x&lt;/td&gt;
    &lt;td&gt;2Tb HD&lt;/td&gt;
    &lt;td&gt;luks encrypted zfs mirror pool &amp;nbsp;&lt;/td&gt;
    &lt;td&gt;encrypted backup storage&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;2x&lt;/td&gt;
    &lt;td&gt;1.5Tb HD&lt;/td&gt;
    &lt;td&gt;zfs mirror pool&lt;/td&gt;
    &lt;td&gt;staging area for nzb downloading&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;1x&lt;/td&gt;
    &lt;td&gt;0.5Tb HD&lt;/td&gt;
    &lt;td&gt;zfs pool&lt;/td&gt;
    &lt;td&gt;working area for nzb downloading&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;4x&lt;/td&gt;
    &lt;td&gt;1Tb HD&lt;/td&gt;
    &lt;td&gt;zfs striped mirror pool&lt;/td&gt;
    &lt;td&gt;general purpose storage (databases, etc.)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;having enabled zfs, now i needed to run my apps.&lt;/p&gt;

&lt;p&gt;i managed to dockerize nzbget and filebot.&lt;/p&gt;

&lt;p&gt;sickbeard was customized based on an image from the docker index&lt;/p&gt;

&lt;h3 id=&#34;profit:7168b982f076d28b250775e790646ac1&#34;&gt;Profit&lt;/h3&gt;

&lt;p&gt;great success !!!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/docker.png&#34; alt=&#34;docker&#34; title=&#34;docker&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/dockerimages.png&#34; alt=&#34;dockerimages&#34; title=&#34;dockerimages&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/htop.png&#34; alt=&#34;htop&#34; title=&#34;htop&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/pool1.png&#34; alt=&#34;pool1&#34; title=&#34;pool1&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/pool2.png&#34; alt=&#34;pool2&#34; title=&#34;pool2&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/zfslist.png&#34; alt=&#34;zfslist&#34; title=&#34;zfslist&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;conclusion:7168b982f076d28b250775e790646ac1&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;i&amp;rsquo;m currently working on a postgresql and gitlab container.&lt;/p&gt;
</description>
    </item>
    
    
    <item>
      <title>hotfix 2</title>
      <link>http://localhost:1313/hotfix-2/</link>
      <pubDate>Mon, 06 Aug 2012 23:23:23 GMT</pubDate>
      
      <guid>http://localhost:1313/hotfix-2/</guid>
      <description>&lt;p&gt;i&amp;rsquo;ve built my esxi box (skynet) &amp;hellip; find some additional info at &lt;a href=&#34;http://lime-technology.com/forum/index.php?topic=21514.0&#34; title=&#34;skynet + hal &amp;amp; wopr (esxi head + 2 unraid das)&#34; target=&#34;_blank&#34;&gt;the unraid forums&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;something i wanted to document. the command sequence to share the encrypted zfs filesystem after boot is&lt;/p&gt;

&lt;blockquote&gt;sudo zfs mount -a (asks for the passphrase) &lt;br/&gt;
sudo zfs set share=name=export_backup,path=/export/backup,prot=nfs backup/data 
&lt;/blockquote&gt;

&lt;p&gt;the latter command is based on my setup
also, oracle recently published an article on &lt;a href=&#34;http://www.oracle.com/technetwork/articles/servers-storage-admin/manage-zfs-encryption-1715034.html&#34;&gt;How to manage ZFS Data Encryption&lt;/a&gt;, with more details about zfs data encryption.&lt;/p&gt;

&lt;p&gt;i haven&amp;rsquo;t processed it yet, but it&amp;rsquo;s worth a read&lt;/p&gt;
</description>
    </item>
    
    
    <item>
      <title>storage wars</title>
      <link>http://localhost:1313/storage-wars/</link>
      <pubDate>Sun, 06 May 2012 23:23:23 GMT</pubDate>
      
      <guid>http://localhost:1313/storage-wars/</guid>
      <description>&lt;p&gt;i&amp;rsquo;m currently running an &lt;a href=&#34;http://lime-technology.com&#34;&gt;unRaid&lt;/a&gt; server, which hosts my media collectionÂ  (movies, tv series, music, pictures and data backups)&lt;/p&gt;

&lt;p&gt;but, i&amp;rsquo;m running out of space on the server (currently 31.69TB used out of 32.51TB available)&lt;/p&gt;

&lt;p&gt;so i started looking for options on how to expand my capacity and decided on adding a second unRaid server, virtualizing both unRaids in an ESXi 5 head server and putting the disks on two DAS boxes (discussions at &lt;a href=&#34;http://lime-technology.com/forum/index.php?topic=14521.0&#34;&gt;two unraid servers&lt;/a&gt;, &lt;a href=&#34;http://lime-technology.com/forum/index.php?topic=14695.0&#34;&gt;atlas: a virtualized unraid&lt;/a&gt;, &lt;a href=&#34;http://www.servethehome.com/sas-expanders-diy-cheap-low-cost-jbod-enclosures-raid/&#34;&gt;building a jbod das&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;at some point during that span of time, zfs came up. i researched as much as i could and finally found this enlightening thread: &lt;a href=&#34;http://forum.xbmc.org/showthread.php?tid=82811&amp;amp;amp;page=17&#34;&gt;Freenas vs unRaid&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;my take on this whole storage wars is that each solution has its merits and my plan changed slightly&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;continue with the virtualized + das boxes unraid setup&lt;/li&gt;
&lt;li&gt;add a virtualized zfs implementation to backup my osx data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;the idea is that unRaid is much better suited to hold an expanding media collection, where i can add drives when needed and keep having one parity disk to protect the array. under zfs, every drive (or more precisely every vdev) would hit me with a parity space penalty&lt;/p&gt;

&lt;p&gt;in my next posts, i will write down my zfs test scenario, while i wait for the parts of my all-encompassing ESXi build&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>